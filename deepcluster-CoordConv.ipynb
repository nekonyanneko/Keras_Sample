{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.misc import comb\n",
    "from munkres import Munkres\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTデータを用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "x = np.divide(x, 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoordConvの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CoordinateChannel(Layer):\n",
    "    \"\"\" Adds Coordinate Channels to the input tensor.\n",
    "    # Arguments\n",
    "        rank: An integer, the rank of the input data-uniform,\n",
    "            e.g. \"2\" for 2D convolution.\n",
    "        use_radius: Boolean flag to determine whether the\n",
    "            radius coordinate should be added for 2D rank\n",
    "            inputs or not.\n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n",
    "            inputs with shape `(batch, channels, ...)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        ND tensor with shape:\n",
    "        `(samples, channels, *)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or ND tensor with shape:\n",
    "        `(samples, *, channels)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "    # Output shape\n",
    "        ND tensor with shape:\n",
    "        `(samples, channels + 2, *)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, *, channels + 2)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "    # References:\n",
    "        - [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rank,\n",
    "                 use_radius=False,\n",
    "                 data_format=None,\n",
    "                 **kwargs):\n",
    "        super(_CoordinateChannel, self).__init__(**kwargs)\n",
    "\n",
    "        if data_format not in [None, 'channels_first', 'channels_last']:\n",
    "            raise ValueError('`data_format` must be either \"channels_last\", \"channels_first\" '\n",
    "                             'or None.')\n",
    "\n",
    "        self.rank = rank\n",
    "        self.use_radius = use_radius\n",
    "        self.data_format = K.image_data_format() if data_format is None else data_format\n",
    "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[self.axis]\n",
    "\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
    "                                    axes={self.axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        input_shape = K.shape(inputs)\n",
    "\n",
    "        if self.rank == 1:\n",
    "            input_shape = [input_shape[i] for i in range(3)]\n",
    "            batch_shape, dim, channels = input_shape\n",
    "\n",
    "            xx_range = K.tile(K.expand_dims(K.arange(0, dim), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            xx_range = K.expand_dims(xx_range, axis=-1)\n",
    "\n",
    "            xx_channels = K.cast(xx_range, K.floatx())\n",
    "            xx_channels = xx_channels / K.cast(dim - 1, K.floatx())\n",
    "            xx_channels = (xx_channels * 2) - 1.\n",
    "\n",
    "            outputs = K.concatenate([inputs, xx_channels], axis=-1)\n",
    "\n",
    "        if self.rank == 2:\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
    "                input_shape = K.shape(inputs)\n",
    "\n",
    "            input_shape = [input_shape[i] for i in range(4)]\n",
    "            batch_shape, dim1, dim2, channels = input_shape\n",
    "\n",
    "            xx_ones = K.ones(K.stack([batch_shape, dim2]), dtype='int32')\n",
    "            xx_ones = K.expand_dims(xx_ones, axis=-1)\n",
    "\n",
    "            xx_range = K.tile(K.expand_dims(K.arange(0, dim1), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            xx_range = K.expand_dims(xx_range, axis=1)\n",
    "            xx_channels = K.batch_dot(xx_ones, xx_range, axes=[2, 1])\n",
    "            xx_channels = K.expand_dims(xx_channels, axis=-1)\n",
    "            xx_channels = K.permute_dimensions(xx_channels, [0, 2, 1, 3])\n",
    "\n",
    "            yy_ones = K.ones(K.stack([batch_shape, dim1]), dtype='int32')\n",
    "            yy_ones = K.expand_dims(yy_ones, axis=1)\n",
    "\n",
    "            yy_range = K.tile(K.expand_dims(K.arange(0, dim2), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            yy_range = K.expand_dims(yy_range, axis=-1)\n",
    "\n",
    "            yy_channels = K.batch_dot(yy_range, yy_ones, axes=[2, 1])\n",
    "            yy_channels = K.expand_dims(yy_channels, axis=-1)\n",
    "            yy_channels = K.permute_dimensions(yy_channels, [0, 2, 1, 3])\n",
    "\n",
    "            xx_channels = K.cast(xx_channels, K.floatx())\n",
    "            xx_channels = xx_channels / K.cast(dim1 - 1, K.floatx())\n",
    "            xx_channels = (xx_channels * 2) - 1.\n",
    "\n",
    "            yy_channels = K.cast(yy_channels, K.floatx())\n",
    "            yy_channels = yy_channels / K.cast(dim2 - 1, K.floatx())\n",
    "            yy_channels = (yy_channels * 2) - 1.\n",
    "\n",
    "            outputs = K.concatenate([inputs, xx_channels, yy_channels], axis=-1)\n",
    "\n",
    "            if self.use_radius:\n",
    "                rr = K.sqrt(K.square(xx_channels - 0.5) +\n",
    "                            K.square(yy_channels - 0.5))\n",
    "                outputs = K.concatenate([outputs, rr], axis=-1)\n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])\n",
    "\n",
    "        if self.rank == 3:\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = K.permute_dimensions(inputs, [0, 2, 3, 4, 1])\n",
    "                input_shape = K.shape(inputs)\n",
    "\n",
    "            input_shape = [input_shape[i] for i in range(5)]\n",
    "            batch_shape, dim1, dim2, dim3, channels = input_shape\n",
    "\n",
    "            xx_ones = K.ones(K.stack([batch_shape, dim3]), dtype='int32')\n",
    "            xx_ones = K.expand_dims(xx_ones, axis=-1)\n",
    "\n",
    "            xx_range = K.tile(K.expand_dims(K.arange(0, dim2), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            xx_range = K.expand_dims(xx_range, axis=1)\n",
    "\n",
    "            xx_channels = K.batch_dot(xx_ones, xx_range, axes=[2, 1])\n",
    "            xx_channels = K.expand_dims(xx_channels, axis=-1)\n",
    "            xx_channels = K.permute_dimensions(xx_channels, [0, 2, 1, 3])\n",
    "\n",
    "            xx_channels = K.expand_dims(xx_channels, axis=1)\n",
    "            xx_channels = K.tile(xx_channels,\n",
    "                                 [1, dim1, 1, 1, 1])\n",
    "\n",
    "            yy_ones = K.ones(K.stack([batch_shape, dim2]), dtype='int32')\n",
    "            yy_ones = K.expand_dims(yy_ones, axis=1)\n",
    "\n",
    "            yy_range = K.tile(K.expand_dims(K.arange(0, dim3), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            yy_range = K.expand_dims(yy_range, axis=-1)\n",
    "\n",
    "            yy_channels = K.batch_dot(yy_range, yy_ones, axes=[2, 1])\n",
    "            yy_channels = K.expand_dims(yy_channels, axis=-1)\n",
    "            yy_channels = K.permute_dimensions(yy_channels, [0, 2, 1, 3])\n",
    "\n",
    "            yy_channels = K.expand_dims(yy_channels, axis=1)\n",
    "            yy_channels = K.tile(yy_channels,\n",
    "                                 [1, dim1, 1, 1, 1])\n",
    "\n",
    "            zz_range = K.tile(K.expand_dims(K.arange(0, dim1), axis=0),\n",
    "                              K.stack([batch_shape, 1]))\n",
    "            zz_range = K.expand_dims(zz_range, axis=-1)\n",
    "            zz_range = K.expand_dims(zz_range, axis=-1)\n",
    "\n",
    "            zz_channels = K.tile(zz_range,\n",
    "                                 [1, 1, dim2, dim3])\n",
    "            zz_channels = K.expand_dims(zz_channels, axis=-1)\n",
    "\n",
    "            xx_channels = K.cast(xx_channels, K.floatx())\n",
    "            xx_channels = xx_channels / K.cast(dim2 - 1, K.floatx())\n",
    "            xx_channels = xx_channels * 2 - 1.\n",
    "\n",
    "            yy_channels = K.cast(yy_channels, K.floatx())\n",
    "            yy_channels = yy_channels / K.cast(dim3 - 1, K.floatx())\n",
    "            yy_channels = yy_channels * 2 - 1.\n",
    "\n",
    "            zz_channels = K.cast(zz_channels, K.floatx())\n",
    "            zz_channels = zz_channels / K.cast(dim1 - 1, K.floatx())\n",
    "            zz_channels = zz_channels * 2 - 1.\n",
    "\n",
    "            outputs = K.concatenate([inputs, zz_channels, xx_channels, yy_channels],\n",
    "                                    axis=-1)\n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 4, 1, 2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[self.axis]\n",
    "\n",
    "        if self.use_radius and self.rank == 2:\n",
    "            channel_count = 3\n",
    "        else:\n",
    "            channel_count = self.rank\n",
    "\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[self.axis] = input_shape[self.axis] + channel_count\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'rank': self.rank,\n",
    "            'use_radius': self.use_radius,\n",
    "            'data_format': self.data_format\n",
    "        }\n",
    "        base_config = super(_CoordinateChannel, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateChannel1D(_CoordinateChannel):\n",
    "    \"\"\" Adds Coordinate Channels to the input tensor of rank 1.\n",
    "    # Arguments\n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n",
    "            inputs with shape `(batch, channels, ...)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(batch_size, steps, input_dim)`\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(batch_size, steps, input_dim + 2)`\n",
    "    # References:\n",
    "        - [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_format=None, **kwargs):\n",
    "        super(CoordinateChannel1D, self).__init__(\n",
    "            rank=1,\n",
    "            use_radius=False,\n",
    "            data_format=data_format,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CoordinateChannel1D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        config.pop('use_radius')\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateChannel2D(_CoordinateChannel):\n",
    "    \"\"\" Adds Coordinate Channels to the input tensor.\n",
    "    # Arguments\n",
    "        use_radius: Boolean flag to determine whether the\n",
    "            radius coordinate should be added for 2D rank\n",
    "            inputs or not.\n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n",
    "            inputs with shape `(batch, channels, ...)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels + 2/3, rows, cols)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels + 2/3)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "        If `use_radius` is set, then will have 3 additional filers,\n",
    "        else only 2 additional filters will be added.\n",
    "    # References:\n",
    "        - [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_radius=False,\n",
    "                 data_format=None,\n",
    "                 **kwargs):\n",
    "        super(CoordinateChannel2D, self).__init__(\n",
    "            rank=2,\n",
    "            use_radius=use_radius,\n",
    "            data_format=data_format,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CoordinateChannel2D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateChannel3D(_CoordinateChannel):\n",
    "    \"\"\" Adds Coordinate Channels to the input tensor.\n",
    "    # Arguments\n",
    "        rank: An integer, the rank of the input data-uniform,\n",
    "            e.g. \"2\" for 2D convolution.\n",
    "        use_radius: Boolean flag to determine whether the\n",
    "            radius coordinate should be added for 2D rank\n",
    "            inputs or not.\n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n",
    "            inputs with shape `(batch, channels, ...)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "    # Output shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, channels + 2, conv_dim1, conv_dim2, conv_dim3)`\n",
    "        if `data_format` is `\"channels_first\"`\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, conv_dim1, conv_dim2, conv_dim3, channels + 2)`\n",
    "        if `data_format` is `\"channels_last\"`.\n",
    "    # References:\n",
    "        - [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_format=None,\n",
    "                 **kwargs):\n",
    "        super(CoordinateChannel3D, self).__init__(\n",
    "            rank=3,\n",
    "            use_radius=False,\n",
    "            data_format=data_format,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CoordinateChannel3D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        config.pop('use_radius')\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_custom_objects().update({'CoordinateChannel1D': CoordinateChannel1D,\n",
    "                             'CoordinateChannel2D': CoordinateChannel2D,\n",
    "                             'CoordinateChannel3D': CoordinateChannel3D})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoderを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    n_stacks = len(dims) - 1\n",
    "    \n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    rx = Reshape((dims[0],1))(input_img)\n",
    "    x = CoordinateChannel1D()(rx)\n",
    "    #x = input_img\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    \n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "    x = Dense(1, kernel_initializer=init, name='decoder_0')(x)\n",
    "    x = Flatten()(x)\n",
    "    decoded = x\n",
    "    \n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [x.shape[-1], 500, 500, 2000, 10]\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "pretrain_epochs = 300\n",
    "batch_size = 256\n",
    "save_dir = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.summary()\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs) #, callbacks=cb)\n",
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClusteringLayerを独自定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepClusterモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: k-meansでクラスタ中心を計算し、初期値とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスタ番号の結果を保存する\n",
    "y_pred_last = np.copy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelに中心値を格納する\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clusterings(labels_true, labels_pred):\n",
    "    \"\"\"Check that the two clusterings matching 1D integer arrays\"\"\"\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    # input checks\n",
    "    if labels_true.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\n",
    "    if labels_pred.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\n",
    "    if labels_true.shape != labels_pred.shape:\n",
    "        raise ValueError(\n",
    "            \"labels_true and labels_pred must have same size, got %d and %d\"\n",
    "            % (labels_true.shape[0], labels_pred.shape[0]))\n",
    "    return labels_true, labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix(labels_true, labels_pred, eps=None):\n",
    "    \"\"\"Build a contengency matrix describing the relationship between labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : int array, shape = [n_samples]\n",
    "        Ground truth class labels to be used as a reference\n",
    "    labels_pred : array, shape = [n_samples]\n",
    "        Cluster labels to evaluate\n",
    "    eps: None or float\n",
    "        If a float, that value is added to all values in the contingency\n",
    "        matrix. This helps to stop NaN propogation.\n",
    "        If ``None``, nothing is adjusted.\n",
    "    Returns\n",
    "    -------\n",
    "    contingency: array, shape=[n_classes_true, n_classes_pred]\n",
    "        Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n",
    "        true class :math:`i` and in predicted class :math:`j`. If\n",
    "        ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n",
    "        given, the dtype will be float.\n",
    "    \"\"\"\n",
    "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "    n_classes = classes.shape[0]\n",
    "    n_clusters = clusters.shape[0]\n",
    "    # Using coo_matrix to accelerate simple histogram calculation,\n",
    "    # i.e. bins are consecutive integers\n",
    "    # Currently, coo_matrix is faster than histogram2d for simple cases\n",
    "    contingency = coo_matrix((np.ones(class_idx.shape[0]),\n",
    "                              (class_idx, cluster_idx)),\n",
    "                             shape=(n_classes, n_clusters),\n",
    "                             dtype=np.int).toarray()\n",
    "    if eps is not None:\n",
    "        # don't use += as contingency is integer\n",
    "        contingency = contingency + eps\n",
    "    return contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_score(labels_true, labels_pred, contingency=None):\n",
    "    \"\"\"Mutual Information between two clusterings\n",
    "    See also\n",
    "    --------\n",
    "    adjusted_mutual_info_score: Adjusted against chance Mutual Information\n",
    "    normalized_mutual_info_score: Normalized Mutual Information\n",
    "    \"\"\"\n",
    "    if contingency is None:\n",
    "        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "        contingency = contingency_matrix(labels_true, labels_pred)\n",
    "    contingency = np.array(contingency, dtype='float')\n",
    "    contingency_sum = np.sum(contingency)\n",
    "    pi = np.sum(contingency, axis=1)\n",
    "    pj = np.sum(contingency, axis=0)\n",
    "    outer = np.outer(pi, pj)\n",
    "    nnz = contingency != 0.0\n",
    "    # normalized contingency\n",
    "    contingency_nm = contingency[nnz]\n",
    "    log_contingency_nm = np.log(contingency_nm)\n",
    "    contingency_nm /= contingency_sum\n",
    "    # log(a / b) should be calculated as log(a) - log(b) for\n",
    "    # possible loss of precision\n",
    "    log_outer = -np.log(outer[nnz]) + log(pi.sum()) + log(pj.sum())\n",
    "    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum))\n",
    "          + contingency_nm * log_outer)\n",
    "    return mi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    \"\"\"Calculates the entropy for a labeling.\"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 1.0\n",
    "    label_idx = np.unique(labels, return_inverse=True)[1]\n",
    "    pi = np.bincount(label_idx).astype(np.float)\n",
    "    pi = pi[pi > 0]\n",
    "    pi_sum = np.sum(pi)\n",
    "    # log(a / b) should be calculated as log(a) - log(b) for\n",
    "    # possible loss of precision\n",
    "    return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb2(n):\n",
    "    # the exact version is faster for k == 2: use it by default globally in\n",
    "    # this module instead of the float approximate variant\n",
    "    return comb(n, 2, exact=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels_true, labels_pred):\n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = labels_true.shape[0]\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "    # Special limit cases: no clustering since the data is not split;\n",
    "    # or trivial clustering where each document is assigned a unique cluster.\n",
    "    # These are perfect matches hence return 1.0.\n",
    "    if (classes.shape[0] == clusters.shape[0] == 1\n",
    "            or classes.shape[0] == clusters.shape[0] == 0\n",
    "            or classes.shape[0] == clusters.shape[0] == len(labels_true)):\n",
    "        return 1.0\n",
    "    \n",
    "    # print \"accuracy testing...\"\n",
    "    contingency = contingency_matrix(labels_true, labels_pred) #Type: <type 'numpy.ndarray'>:rows are clusters, cols are classes\n",
    "    contingency = -contingency\n",
    "    #print contingency\n",
    "    contingency = contingency.tolist()\n",
    "    m = Munkres() # Best mapping by using Kuhn-Munkres algorithm\n",
    "    map_pairs = m.compute(contingency) #best match to find the minimum cost\n",
    "    sum_value = 0\n",
    "    for key,value in map_pairs:\n",
    "        sum_value = sum_value + contingency[key][value]\n",
    "    \n",
    "    return float(-sum_value)/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mutual_info_score(labels_true, labels_pred):\n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "    # Special limit cases: no clustering since the data is not split.\n",
    "    # This is a perfect match hence return 1.0.\n",
    "    if (classes.shape[0] == clusters.shape[0] == 1\n",
    "            or classes.shape[0] == clusters.shape[0] == 0):\n",
    "        return 1.0\n",
    "    contingency = contingency_matrix(labels_true, labels_pred)\n",
    "    contingency = np.array(contingency, dtype='float')\n",
    "    # Calculate the MI for the two clusterings\n",
    "    mi = mutual_info_score(labels_true, labels_pred,\n",
    "                           contingency=contingency)\n",
    "    # Calculate the expected value for the mutual information\n",
    "    # Calculate entropy for each labeling\n",
    "    h_true, h_pred = entropy(labels_true), entropy(labels_pred)\n",
    "    nmi = mi / max(np.sqrt(h_true * h_pred), 1e-10)\n",
    "    return nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_rand_score(labels_true, labels_pred):\n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = labels_true.shape[0]\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "    # Special limit cases: no clustering since the data is not split;\n",
    "    # or trivial clustering where each document is assigned a unique cluster.\n",
    "    # These are perfect matches hence return 1.0.\n",
    "    if (classes.shape[0] == clusters.shape[0] == 1\n",
    "            or classes.shape[0] == clusters.shape[0] == 0\n",
    "            or classes.shape[0] == clusters.shape[0] == len(labels_true)):\n",
    "        return 1.0\n",
    "\n",
    "    contingency = contingency_matrix(labels_true, labels_pred)\n",
    "    #print contingency\n",
    "    # Compute the ARI using the contingency data\n",
    "    #print \"contingency.sum(axis=1) = %s\" %(str(contingency.sum(axis=1)))\n",
    "    sum_comb_c = sum(comb2(n_c) for n_c in contingency.sum(axis=1)) # TP+FP\n",
    "    #print \"sum_comb_c = %d\" %(sum_comb_c)\n",
    "    \n",
    "    #print \"contingency.sum(axis=0) = %s\" %(str(contingency.sum(axis=0)))\n",
    "    sum_comb_k = sum(comb2(n_k) for n_k in contingency.sum(axis=0)) #TP+FN\n",
    "    #print \"sum_comb_k = %d\" %(sum_comb_k)\n",
    "    \n",
    "    #print \"contingency.flatten() = %s\" %(str(contingency.flatten()))\n",
    "    sum_comb = sum(comb2(n_ij) for n_ij in contingency.flatten()) #TP\n",
    "    #print \"sum_comb = %d\" %(sum_comb)\n",
    "    \n",
    "    prod_comb = (sum_comb_c * sum_comb_k) / float(comb(n_samples, 2))\n",
    "    mean_comb = (sum_comb_k + sum_comb_c) / 2.\n",
    "    \n",
    "    return ((sum_comb - prod_comb) / (mean_comb - prod_comb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 8000\n",
    "update_interval = 140\n",
    "index_array = np.arange(x.shape[0])\n",
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(accuracy(y, y_pred), 5)\n",
    "            nmi = np.round(normalized_mutual_info_score(y, y_pred), 5)\n",
    "            ari = np.round(adjusted_rand_score(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion - model convergence\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/DEC_model_weight_final.h5')\n",
    "model.save(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=3)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
